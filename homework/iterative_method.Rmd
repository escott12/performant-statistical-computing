---
title: 'Homework: iterative method'
output:
  html_document:
    df_print: paged
---

```{r setup, include = FALSE}
source(file.path("..", "R", "util.R"))
required_packages <- c("RSpectra")
install_and_load_packages(required_packages)
```

# Problem 1
Auto regressive processes can be viewed as a discrete analog of Ornsteinâ€“Uhlenbeck process &mdash; which coincides with Gaussian process based on an exponential covariance matrix &mdash; and hence is an example of Gaussian Markov random fields.
For instance, stationary lag-1 auto-regressive process 
$$x_t = \phi x_{t - 1} + \sqrt{1 - \phi^2} \, \epsilon_t, 
  \quad x_0 \sim \mathcal{N}(0, 1), 
  \quad \epsilon_t \mathbin{\overset{\small \textrm{i.i.d.}}{\sim}} \mathcal{N}(0, 1)$$
has the _tri-diagonal_ precision matrix
$$\boldsymbol{\Sigma}^{-1} = \frac{1}{1 - \phi^2} 
  \begin{bmatrix} 
  1 & -\phi & 0 & & & \ldots & 0 \\
  -\phi & 1 + \phi^2 & -\phi & 0 & & & \vdots \\
  0 & -\phi & 1 + \phi^2 & -\phi & 0 & & \\
    & & \ddots & \ddots & \ddots & & \\
    & & 0 & -\phi & 1 + \phi^2 & -\phi & 0 \\
  \vdots & &   & 0 & -\phi & 1 + \phi^2 & -\phi \\
  0 & \ldots &   &   & 0 & -\phi & 1\\
  \end{bmatrix}.$$
More generally, a lag-$k$ (non-stationary) auto-regressive process has a _banded_ precision matrix with bandwidth $k$.

Implement a fast matrix-vector $\boldsymbol{v} \to \boldsymbol{\Sigma}^{-1} \boldsymbol{v}$ operation, exploiting the structure of the AR-1 precision matrix.
Then use this function to find the top 10 principal components of $\boldsymbol{\Sigma}$ (not $\boldsymbol{\Sigma}^{-1}$) via Lanczos algorithm provided via `RSpectra::eigs_sym`.

```{r}
ar_length <- 4096
auto_corr <- .9 # Corresponds to `\phi` above

ar_precision_matvec <- function(v, auto_corr) {
  # Fill in: note that you can vectorize the calculation and do *not* need a for-loop.
  # (Hint: how would you efficiently carry out a matrix-vector operation if the matrix has non-zero entries only along the sub or super diagonal?)

  # Dimensions of square matrix
  n <- sqrt(ar_length)

  # Vectorize
  d <- c(1, rep(1 + auto_corr^2, n - 2), 1)
  d_sup <- rep(-auto_corr, n - 1)
  d_sub <- rep(-auto_corr, n - 1)

  # Calculate 3 element-wise products
  vec1 <- d * v
  vec2 <- c(d_sup * v[-1], 0)
  vec3 <- c(0, d_sub * v[-n])

  # Multiply Sigma_inv %*% v
  1 / (1 - auto_corr^2) * (vec1 + vec2 + vec3)
}

ar_eig <- eigs_sym(
  ar_precision_matvec,
  args = auto_corr,
  n = sqrt(ar_length),
  k = 10,
  opts = list(
    ncv = 50, # Spectrum distribution of AR-1 process is not very spread out on the extreme ends and is actually a hard case for Lanczos. So it helps to have more Lanczos vectors than the default for faster convergence.
    maxitr = 10^3, # Cap it just in case
    retvec = TRUE # More efficient to do without eigenvectors when not needed
  ),
)

# Try to apply eigs_sym to Sigma matrix input (rather than function input) 
corr_mat <- matrix(data = NA, nrow = sqrt(ar_length), ncol = sqrt(ar_length))
corr_mat <- auto_corr^(abs(row(corr_mat) - col(corr_mat)))
ar_eig_matrixA <- eigs_sym(
  A = corr_mat,
  n = sqrt(ar_length),
  k = 10)

# These results match the direct approach from later. Looks like we're not using eigs_sym correctly above to find eigenvalues of Sigma (we're finding eigenvalues of Sigma inverse)
# If we just want the 10 largest eigenvalues for Sigma, we could get the 10 smallest for Sigma inv and invert them. 
ar_eig <- eigs_sym(
  ar_precision_matvec,
  args = auto_corr,
  n = sqrt(ar_length),
  k = 10,
  opts = list(
    ncv = 50, # Spectrum distribution of AR-1 process is not very spread out on the extreme ends and is actually a hard case for Lanczos. So it helps to have more Lanczos vectors than the default for faster convergence.
    maxitr = 10^3, # Cap it just in case
    retvec = TRUE, # More efficient to do without eigenvectors when not needed
    which = "SM"
  ),
)
1/ar_eig$values[10:1]
```

Now, directly compute the eigen decomposition of $\boldsymbol{\Sigma}$ (not $\boldsymbol{\Sigma}^{-1}$) and compare its output with the principal components and associated variances found via Lancsoz algorithm.

```{r}
# Store correlation matrix
corr_mat <- matrix(data = NA, nrow = sqrt(ar_length), ncol = sqrt(ar_length))
corr_mat <- auto_corr^(abs(row(corr_mat) - col(corr_mat)))

# Store precision matrix
prec_mat <- diag(x = 1 + auto_corr^2, nrow = sqrt(ar_length), ncol = sqrt(ar_length))
prec_mat[c(1, ar_length)] <- 1
prec_mat[abs(row(prec_mat) - col(prec_mat)) == 1] <- -auto_corr
prec_mat <- 1 / (1 - auto_corr^2) * prec_mat

# Calculate eigenvectors directly (to check things later)
eig_vals_Sigma <- eigen(corr_mat)$values
eig_vecs_Sigma <- eigen(corr_mat)$vectors
eig_vals_Sigma_inv <- eigen(prec_mat)$values
eig_vecs_Sigma_inv <- eigen(prec_mat)$vectors

data.frame("direct_sigma" = head(eig_vals_Sigma, n = 10),
           "Inverted_direct_sigma_inv" = tail(1/eig_vals_Sigma_inv, n = 10)[10:1],
           "eigs_symm_matrix_form" = ar_eig_matrixA$values[1:10],
           "eigs_symm_fx_form" = 1/ar_eig$values[10:1])
```

```{r}
# Test vectorized multiplication from above
v <- rnorm(64)
data.frame("fx_output" = ar_precision_matvec(v = v, auto_corr = 0.9), "direct" = prec_mat %*% v)
```


**Remark:** 
For banded matrices, there actually are even more efficient approaches.
To get a sense of special routines available for banded matrices, you can take a look at `*_banded` functions in [SciPy's linear algebra routines](https://docs.scipy.org/doc/scipy/reference/linalg.html).
Even those functions represent only a subset of available numerical linear algebra techniques; see
[LAPACK documentation for SVD](https://www.netlib.org/lapack/lug/node32.html) for example.
